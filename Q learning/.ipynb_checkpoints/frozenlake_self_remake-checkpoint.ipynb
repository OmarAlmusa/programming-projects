{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ec5fb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import gym.spaces\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "890159e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_neurons = 128\n",
    "percentile = 30\n",
    "GAMMA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cf74e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode_Step = namedtuple('Episode_Step', ['observations', 'actions'])\n",
    "Episode = namedtuple('Episode', ['rewards', 'episode_step'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f2852f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotencoder(x, highest_number):\n",
    "    onehotencoded_variable = np.zeros(highest_number, dtype=float)\n",
    "    onehotencoded_variable[x] = 1\n",
    "    return onehotencoded_variable\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af3ef104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OHE(tensor, h):\n",
    "    z0 = np.zeros([len(tensor), h])\n",
    "    for t in tensor:\n",
    "        z0[int(t), :] = onehotencoder(int(t), h)\n",
    "    return z0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cecf51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self, obs_size, n_neurons, n_actions):\n",
    "        super(model, self).__init__()\n",
    "        \n",
    "        self.pipe = nn.Sequential(\n",
    "            nn.Linear(obs_size, n_neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_neurons, n_actions)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.pipe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1ea8bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batchs(env, model, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    \n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor(torch.tensor(obs).unsqueeze(0))\n",
    "        actions_prob_v = sm(model(obs_v))\n",
    "        actions_prob = actions_prob_v.data.numpy()[0]\n",
    "        \n",
    "        action = np.random.choice(len(actions_prob), p=actions_prob)\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        steps.append(Episode_Step(observations = obs, actions = action))\n",
    "        \n",
    "        if is_done:\n",
    "            batch.append(Episode(rewards=episode_reward, episode_step=steps))\n",
    "            episode_reward = 0.0\n",
    "            next_obs = env.reset()\n",
    "            steps = []\n",
    "            \n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        \n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbab88c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch, percentile):\n",
    "    filter_fun = lambda s: s.reward * (GAMMA ** len(s.steps))\n",
    "    disc_rewards = list(map(filter_fun, batch))\n",
    "    reward_bound = np.percentile(disc_rewards, percentile)\n",
    "    \n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    elite_batch = []\n",
    "    for example, discounted_reward in zip(batch, disc_rewards):\n",
    "        if discounted_reward > reward_bound:\n",
    "            train_obs.extend(map(lambda step: step.observations, example.episode_step))\n",
    "            train_act.extend(map(lambda step: step.actions, example.episode_step))\n",
    "            elite_batch.append(example)\n",
    "    \n",
    "    return elite_batch, train_obs, train_act, reward_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45d7fab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        \n",
    "        shape = (env.observation_space.n, )\n",
    "        self.observation_space = gym.spaces.Box(0.0, 0.1, shape, dtype=np.float32)\n",
    "    \n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1903f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caf73105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=1.397, reward_mean=0.0, reward_bound=0.0\n",
      "1: loss=1.381, reward_mean=0.1, reward_bound=0.0\n",
      "2: loss=1.356, reward_mean=0.0, reward_bound=0.0\n",
      "3: loss=1.338, reward_mean=0.0, reward_bound=0.0\n",
      "4: loss=1.347, reward_mean=0.0, reward_bound=0.0\n",
      "5: loss=1.287, reward_mean=0.1, reward_bound=0.0\n",
      "6: loss=1.320, reward_mean=0.0, reward_bound=0.0\n",
      "7: loss=1.328, reward_mean=0.1, reward_bound=0.0\n",
      "8: loss=1.249, reward_mean=0.0, reward_bound=0.0\n",
      "9: loss=1.185, reward_mean=0.0, reward_bound=0.0\n",
      "10: loss=1.285, reward_mean=0.0, reward_bound=0.0\n",
      "11: loss=1.199, reward_mean=0.0, reward_bound=0.0\n",
      "12: loss=1.197, reward_mean=0.0, reward_bound=0.0\n",
      "13: loss=1.172, reward_mean=0.0, reward_bound=0.0\n",
      "14: loss=1.271, reward_mean=0.0, reward_bound=0.0\n",
      "15: loss=1.214, reward_mean=0.0, reward_bound=0.0\n",
      "16: loss=1.243, reward_mean=0.0, reward_bound=0.0\n",
      "17: loss=1.196, reward_mean=0.1, reward_bound=0.0\n",
      "18: loss=1.186, reward_mean=0.1, reward_bound=0.0\n",
      "19: loss=1.082, reward_mean=0.0, reward_bound=0.0\n",
      "20: loss=1.200, reward_mean=0.0, reward_bound=0.0\n",
      "21: loss=1.276, reward_mean=0.0, reward_bound=0.0\n",
      "22: loss=1.248, reward_mean=0.1, reward_bound=0.0\n",
      "23: loss=1.231, reward_mean=0.0, reward_bound=0.0\n",
      "24: loss=1.283, reward_mean=0.0, reward_bound=0.0\n",
      "25: loss=1.210, reward_mean=0.0, reward_bound=0.0\n",
      "26: loss=1.246, reward_mean=0.0, reward_bound=0.0\n",
      "27: loss=1.213, reward_mean=0.0, reward_bound=0.0\n",
      "28: loss=1.173, reward_mean=0.0, reward_bound=0.0\n",
      "29: loss=1.261, reward_mean=0.1, reward_bound=0.0\n",
      "30: loss=1.238, reward_mean=0.1, reward_bound=0.0\n",
      "31: loss=1.198, reward_mean=0.0, reward_bound=0.0\n",
      "32: loss=1.209, reward_mean=0.0, reward_bound=0.0\n",
      "33: loss=1.290, reward_mean=0.0, reward_bound=0.0\n",
      "34: loss=1.238, reward_mean=0.0, reward_bound=0.0\n",
      "35: loss=1.182, reward_mean=0.0, reward_bound=0.0\n",
      "36: loss=1.202, reward_mean=0.0, reward_bound=0.0\n",
      "37: loss=1.222, reward_mean=0.0, reward_bound=0.0\n",
      "38: loss=1.271, reward_mean=0.1, reward_bound=0.0\n",
      "39: loss=1.130, reward_mean=0.1, reward_bound=0.0\n",
      "40: loss=1.199, reward_mean=0.0, reward_bound=0.0\n",
      "41: loss=1.086, reward_mean=0.0, reward_bound=0.0\n",
      "42: loss=1.215, reward_mean=0.0, reward_bound=0.0\n",
      "43: loss=1.165, reward_mean=0.0, reward_bound=0.0\n",
      "44: loss=1.094, reward_mean=0.0, reward_bound=0.0\n",
      "45: loss=1.146, reward_mean=0.0, reward_bound=0.0\n",
      "46: loss=1.091, reward_mean=0.0, reward_bound=0.0\n",
      "47: loss=1.184, reward_mean=0.0, reward_bound=0.0\n",
      "48: loss=1.125, reward_mean=0.0, reward_bound=0.0\n",
      "49: loss=1.065, reward_mean=0.1, reward_bound=0.0\n",
      "50: loss=1.105, reward_mean=0.1, reward_bound=0.0\n",
      "51: loss=1.031, reward_mean=0.1, reward_bound=0.0\n",
      "52: loss=1.042, reward_mean=0.0, reward_bound=0.0\n",
      "53: loss=1.029, reward_mean=0.2, reward_bound=0.0\n",
      "54: loss=1.069, reward_mean=0.0, reward_bound=0.0\n",
      "55: loss=0.970, reward_mean=0.0, reward_bound=0.0\n",
      "56: loss=0.895, reward_mean=0.0, reward_bound=0.0\n",
      "57: loss=1.013, reward_mean=0.2, reward_bound=0.0\n",
      "58: loss=0.818, reward_mean=0.0, reward_bound=0.0\n",
      "59: loss=0.967, reward_mean=0.2, reward_bound=0.0\n",
      "60: loss=0.829, reward_mean=0.1, reward_bound=0.0\n",
      "61: loss=0.826, reward_mean=0.1, reward_bound=0.0\n",
      "62: loss=0.741, reward_mean=0.0, reward_bound=0.0\n",
      "63: loss=0.872, reward_mean=0.1, reward_bound=0.0\n",
      "64: loss=0.762, reward_mean=0.2, reward_bound=0.0\n",
      "65: loss=0.799, reward_mean=0.1, reward_bound=0.0\n",
      "66: loss=0.627, reward_mean=0.0, reward_bound=0.0\n",
      "67: loss=0.592, reward_mean=0.1, reward_bound=0.0\n",
      "68: loss=0.658, reward_mean=0.0, reward_bound=0.0\n",
      "69: loss=0.667, reward_mean=0.1, reward_bound=0.0\n",
      "70: loss=0.717, reward_mean=0.0, reward_bound=0.0\n",
      "71: loss=0.712, reward_mean=0.0, reward_bound=0.0\n",
      "72: loss=0.690, reward_mean=0.1, reward_bound=0.0\n",
      "73: loss=0.566, reward_mean=0.0, reward_bound=0.0\n",
      "74: loss=0.641, reward_mean=0.1, reward_bound=0.0\n",
      "75: loss=0.777, reward_mean=0.0, reward_bound=0.0\n",
      "76: loss=0.824, reward_mean=0.0, reward_bound=0.0\n",
      "77: loss=0.754, reward_mean=0.0, reward_bound=0.0\n",
      "78: loss=0.759, reward_mean=0.1, reward_bound=0.0\n",
      "79: loss=0.792, reward_mean=0.0, reward_bound=0.0\n",
      "80: loss=0.716, reward_mean=0.0, reward_bound=0.0\n",
      "81: loss=0.809, reward_mean=0.1, reward_bound=0.0\n",
      "82: loss=0.764, reward_mean=0.2, reward_bound=0.0\n",
      "83: loss=0.826, reward_mean=0.0, reward_bound=0.0\n",
      "84: loss=0.741, reward_mean=0.0, reward_bound=0.0\n",
      "85: loss=0.852, reward_mean=0.0, reward_bound=0.0\n",
      "86: loss=0.959, reward_mean=0.0, reward_bound=0.0\n",
      "87: loss=0.857, reward_mean=0.0, reward_bound=0.0\n",
      "88: loss=0.927, reward_mean=0.0, reward_bound=0.0\n",
      "89: loss=0.789, reward_mean=0.0, reward_bound=0.0\n",
      "90: loss=0.784, reward_mean=0.0, reward_bound=0.0\n",
      "91: loss=1.021, reward_mean=0.0, reward_bound=0.0\n",
      "92: loss=0.778, reward_mean=0.0, reward_bound=0.0\n",
      "93: loss=0.998, reward_mean=0.1, reward_bound=0.0\n",
      "94: loss=0.912, reward_mean=0.1, reward_bound=0.0\n",
      "95: loss=0.872, reward_mean=0.0, reward_bound=0.0\n",
      "96: loss=0.926, reward_mean=0.0, reward_bound=0.0\n",
      "97: loss=0.803, reward_mean=0.1, reward_bound=0.0\n",
      "98: loss=0.938, reward_mean=0.1, reward_bound=0.0\n",
      "99: loss=0.978, reward_mean=0.1, reward_bound=0.0\n",
      "100: loss=0.798, reward_mean=0.1, reward_bound=0.0\n",
      "101: loss=0.835, reward_mean=0.0, reward_bound=0.0\n",
      "102: loss=0.811, reward_mean=0.0, reward_bound=0.0\n",
      "103: loss=0.903, reward_mean=0.1, reward_bound=0.0\n",
      "104: loss=0.944, reward_mean=0.0, reward_bound=0.0\n",
      "105: loss=0.861, reward_mean=0.0, reward_bound=0.0\n",
      "106: loss=0.817, reward_mean=0.1, reward_bound=0.0\n",
      "107: loss=0.948, reward_mean=0.0, reward_bound=0.0\n",
      "108: loss=0.870, reward_mean=0.0, reward_bound=0.0\n",
      "109: loss=0.996, reward_mean=0.0, reward_bound=0.0\n",
      "110: loss=1.032, reward_mean=0.1, reward_bound=0.0\n",
      "111: loss=0.939, reward_mean=0.0, reward_bound=0.0\n",
      "112: loss=0.911, reward_mean=0.0, reward_bound=0.0\n",
      "113: loss=1.001, reward_mean=0.0, reward_bound=0.0\n",
      "114: loss=0.838, reward_mean=0.1, reward_bound=0.0\n",
      "115: loss=0.909, reward_mean=0.1, reward_bound=0.0\n",
      "116: loss=0.925, reward_mean=0.1, reward_bound=0.0\n",
      "117: loss=0.975, reward_mean=0.1, reward_bound=0.0\n",
      "118: loss=0.954, reward_mean=0.0, reward_bound=0.0\n",
      "119: loss=0.900, reward_mean=0.1, reward_bound=0.0\n",
      "120: loss=1.028, reward_mean=0.0, reward_bound=0.0\n",
      "121: loss=0.926, reward_mean=0.0, reward_bound=0.0\n",
      "122: loss=0.940, reward_mean=0.2, reward_bound=0.0\n",
      "123: loss=0.863, reward_mean=0.0, reward_bound=0.0\n",
      "124: loss=0.900, reward_mean=0.1, reward_bound=0.0\n",
      "125: loss=1.046, reward_mean=0.0, reward_bound=0.0\n",
      "126: loss=0.781, reward_mean=0.0, reward_bound=0.0\n",
      "127: loss=0.826, reward_mean=0.0, reward_bound=0.0\n",
      "128: loss=0.850, reward_mean=0.1, reward_bound=0.0\n",
      "129: loss=1.001, reward_mean=0.1, reward_bound=0.0\n",
      "130: loss=0.953, reward_mean=0.0, reward_bound=0.0\n",
      "131: loss=0.831, reward_mean=0.0, reward_bound=0.0\n",
      "132: loss=0.858, reward_mean=0.1, reward_bound=0.0\n",
      "133: loss=0.951, reward_mean=0.0, reward_bound=0.0\n",
      "134: loss=0.918, reward_mean=0.0, reward_bound=0.0\n",
      "135: loss=0.832, reward_mean=0.1, reward_bound=0.0\n",
      "136: loss=1.095, reward_mean=0.0, reward_bound=0.0\n",
      "137: loss=0.759, reward_mean=0.1, reward_bound=0.0\n",
      "138: loss=0.822, reward_mean=0.0, reward_bound=0.0\n",
      "139: loss=0.936, reward_mean=0.0, reward_bound=0.0\n",
      "140: loss=0.911, reward_mean=0.0, reward_bound=0.0\n",
      "141: loss=0.952, reward_mean=0.0, reward_bound=0.0\n",
      "142: loss=0.813, reward_mean=0.0, reward_bound=0.0\n",
      "143: loss=1.069, reward_mean=0.0, reward_bound=0.0\n",
      "144: loss=0.990, reward_mean=0.0, reward_bound=0.0\n",
      "145: loss=0.861, reward_mean=0.0, reward_bound=0.0\n",
      "146: loss=1.014, reward_mean=0.1, reward_bound=0.0\n",
      "147: loss=0.964, reward_mean=0.0, reward_bound=0.0\n",
      "148: loss=0.998, reward_mean=0.0, reward_bound=0.0\n",
      "149: loss=0.860, reward_mean=0.0, reward_bound=0.0\n",
      "150: loss=1.023, reward_mean=0.1, reward_bound=0.0\n",
      "151: loss=0.965, reward_mean=0.0, reward_bound=0.0\n",
      "152: loss=0.994, reward_mean=0.0, reward_bound=0.0\n",
      "153: loss=1.056, reward_mean=0.0, reward_bound=0.0\n",
      "154: loss=1.071, reward_mean=0.1, reward_bound=0.0\n",
      "155: loss=1.155, reward_mean=0.0, reward_bound=0.0\n",
      "156: loss=1.026, reward_mean=0.0, reward_bound=0.0\n",
      "157: loss=0.921, reward_mean=0.1, reward_bound=0.0\n",
      "158: loss=1.046, reward_mean=0.1, reward_bound=0.0\n",
      "159: loss=0.979, reward_mean=0.1, reward_bound=0.0\n",
      "160: loss=0.988, reward_mean=0.0, reward_bound=0.0\n",
      "161: loss=1.130, reward_mean=0.0, reward_bound=0.0\n",
      "162: loss=1.101, reward_mean=0.0, reward_bound=0.0\n",
      "163: loss=1.061, reward_mean=0.1, reward_bound=0.0\n",
      "164: loss=0.950, reward_mean=0.2, reward_bound=0.0\n",
      "165: loss=0.937, reward_mean=0.0, reward_bound=0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166: loss=0.921, reward_mean=0.1, reward_bound=0.0\n",
      "167: loss=1.049, reward_mean=0.1, reward_bound=0.0\n",
      "168: loss=1.024, reward_mean=0.1, reward_bound=0.0\n",
      "169: loss=1.112, reward_mean=0.0, reward_bound=0.0\n",
      "170: loss=0.927, reward_mean=0.1, reward_bound=0.0\n",
      "171: loss=0.980, reward_mean=0.0, reward_bound=0.0\n",
      "172: loss=1.129, reward_mean=0.0, reward_bound=0.0\n",
      "173: loss=1.015, reward_mean=0.0, reward_bound=0.0\n",
      "174: loss=1.010, reward_mean=0.0, reward_bound=0.0\n",
      "175: loss=1.100, reward_mean=0.0, reward_bound=0.0\n",
      "176: loss=1.091, reward_mean=0.0, reward_bound=0.0\n",
      "177: loss=1.005, reward_mean=0.1, reward_bound=0.0\n",
      "178: loss=1.096, reward_mean=0.0, reward_bound=0.0\n",
      "179: loss=1.038, reward_mean=0.0, reward_bound=0.0\n",
      "180: loss=0.955, reward_mean=0.0, reward_bound=0.0\n",
      "181: loss=1.008, reward_mean=0.0, reward_bound=0.0\n",
      "182: loss=1.144, reward_mean=0.0, reward_bound=0.0\n",
      "183: loss=0.968, reward_mean=0.0, reward_bound=0.0\n",
      "184: loss=1.053, reward_mean=0.1, reward_bound=0.0\n",
      "185: loss=0.878, reward_mean=0.0, reward_bound=0.0\n",
      "186: loss=1.023, reward_mean=0.1, reward_bound=0.0\n",
      "187: loss=0.977, reward_mean=0.0, reward_bound=0.0\n",
      "188: loss=0.937, reward_mean=0.1, reward_bound=0.0\n",
      "189: loss=0.931, reward_mean=0.1, reward_bound=0.0\n",
      "190: loss=0.848, reward_mean=0.0, reward_bound=0.0\n",
      "191: loss=0.962, reward_mean=0.0, reward_bound=0.0\n",
      "192: loss=0.743, reward_mean=0.0, reward_bound=0.0\n",
      "193: loss=1.003, reward_mean=0.0, reward_bound=0.0\n",
      "194: loss=0.846, reward_mean=0.1, reward_bound=0.0\n",
      "195: loss=0.775, reward_mean=0.0, reward_bound=0.0\n",
      "196: loss=0.889, reward_mean=0.1, reward_bound=0.0\n",
      "197: loss=0.618, reward_mean=0.1, reward_bound=0.0\n",
      "198: loss=0.822, reward_mean=0.0, reward_bound=0.0\n",
      "199: loss=0.598, reward_mean=0.0, reward_bound=0.0\n",
      "200: loss=0.661, reward_mean=0.1, reward_bound=0.0\n",
      "201: loss=0.780, reward_mean=0.1, reward_bound=0.0\n",
      "202: loss=0.524, reward_mean=0.0, reward_bound=0.0\n",
      "203: loss=0.698, reward_mean=0.1, reward_bound=0.0\n",
      "204: loss=0.584, reward_mean=0.1, reward_bound=0.0\n",
      "205: loss=0.640, reward_mean=0.1, reward_bound=0.0\n",
      "206: loss=0.546, reward_mean=0.1, reward_bound=0.0\n",
      "207: loss=0.613, reward_mean=0.0, reward_bound=0.0\n",
      "208: loss=0.519, reward_mean=0.0, reward_bound=0.0\n",
      "209: loss=0.401, reward_mean=0.0, reward_bound=0.0\n",
      "210: loss=0.537, reward_mean=0.1, reward_bound=0.0\n",
      "211: loss=0.547, reward_mean=0.1, reward_bound=0.0\n",
      "212: loss=0.353, reward_mean=0.0, reward_bound=0.0\n",
      "213: loss=0.482, reward_mean=0.0, reward_bound=0.0\n",
      "214: loss=0.456, reward_mean=0.1, reward_bound=0.0\n",
      "215: loss=0.488, reward_mean=0.1, reward_bound=0.0\n",
      "216: loss=0.563, reward_mean=0.1, reward_bound=0.0\n",
      "217: loss=0.446, reward_mean=0.1, reward_bound=0.0\n",
      "218: loss=0.311, reward_mean=0.0, reward_bound=0.0\n",
      "219: loss=0.526, reward_mean=0.0, reward_bound=0.0\n",
      "220: loss=0.615, reward_mean=0.0, reward_bound=0.0\n",
      "221: loss=0.587, reward_mean=0.1, reward_bound=0.0\n",
      "222: loss=0.576, reward_mean=0.0, reward_bound=0.0\n",
      "223: loss=0.510, reward_mean=0.0, reward_bound=0.0\n",
      "224: loss=0.419, reward_mean=0.1, reward_bound=0.0\n",
      "225: loss=0.523, reward_mean=0.0, reward_bound=0.0\n",
      "226: loss=0.516, reward_mean=0.0, reward_bound=0.0\n",
      "227: loss=0.496, reward_mean=0.0, reward_bound=0.0\n",
      "228: loss=0.558, reward_mean=0.1, reward_bound=0.0\n",
      "229: loss=0.450, reward_mean=0.0, reward_bound=0.0\n",
      "230: loss=0.683, reward_mean=0.0, reward_bound=0.0\n",
      "231: loss=0.690, reward_mean=0.0, reward_bound=0.0\n",
      "232: loss=0.554, reward_mean=0.1, reward_bound=0.0\n",
      "233: loss=0.604, reward_mean=0.1, reward_bound=0.0\n",
      "234: loss=0.623, reward_mean=0.0, reward_bound=0.0\n",
      "235: loss=0.659, reward_mean=0.1, reward_bound=0.0\n",
      "236: loss=0.664, reward_mean=0.0, reward_bound=0.0\n",
      "237: loss=0.876, reward_mean=0.0, reward_bound=0.0\n",
      "238: loss=0.655, reward_mean=0.0, reward_bound=0.0\n",
      "239: loss=0.695, reward_mean=0.1, reward_bound=0.0\n",
      "240: loss=0.916, reward_mean=0.1, reward_bound=0.0\n",
      "241: loss=0.806, reward_mean=0.0, reward_bound=0.0\n",
      "242: loss=0.622, reward_mean=0.1, reward_bound=0.0\n",
      "243: loss=0.645, reward_mean=0.2, reward_bound=0.0\n",
      "244: loss=0.754, reward_mean=0.1, reward_bound=0.0\n",
      "245: loss=0.770, reward_mean=0.1, reward_bound=0.0\n",
      "246: loss=0.805, reward_mean=0.0, reward_bound=0.0\n",
      "247: loss=0.838, reward_mean=0.1, reward_bound=0.0\n",
      "248: loss=0.884, reward_mean=0.0, reward_bound=0.0\n",
      "249: loss=0.826, reward_mean=0.1, reward_bound=0.0\n",
      "250: loss=0.676, reward_mean=0.0, reward_bound=0.0\n",
      "251: loss=0.866, reward_mean=0.0, reward_bound=0.0\n",
      "252: loss=0.752, reward_mean=0.0, reward_bound=0.0\n",
      "253: loss=0.821, reward_mean=0.1, reward_bound=0.0\n",
      "254: loss=0.922, reward_mean=0.0, reward_bound=0.0\n",
      "255: loss=0.801, reward_mean=0.0, reward_bound=0.0\n",
      "256: loss=0.667, reward_mean=0.1, reward_bound=0.0\n",
      "257: loss=0.916, reward_mean=0.0, reward_bound=0.0\n",
      "258: loss=0.879, reward_mean=0.0, reward_bound=0.0\n",
      "259: loss=0.916, reward_mean=0.0, reward_bound=0.0\n",
      "260: loss=0.877, reward_mean=0.1, reward_bound=0.0\n",
      "261: loss=0.777, reward_mean=0.0, reward_bound=0.0\n",
      "262: loss=0.805, reward_mean=0.0, reward_bound=0.0\n",
      "263: loss=0.814, reward_mean=0.1, reward_bound=0.0\n",
      "264: loss=0.898, reward_mean=0.1, reward_bound=0.0\n",
      "265: loss=0.948, reward_mean=0.0, reward_bound=0.0\n",
      "266: loss=0.922, reward_mean=0.0, reward_bound=0.0\n",
      "267: loss=0.897, reward_mean=0.0, reward_bound=0.0\n",
      "268: loss=0.906, reward_mean=0.1, reward_bound=0.0\n",
      "269: loss=1.021, reward_mean=0.1, reward_bound=0.0\n",
      "270: loss=1.156, reward_mean=0.0, reward_bound=0.0\n",
      "271: loss=0.910, reward_mean=0.1, reward_bound=0.0\n",
      "272: loss=0.833, reward_mean=0.0, reward_bound=0.0\n",
      "273: loss=0.963, reward_mean=0.0, reward_bound=0.0\n",
      "274: loss=0.926, reward_mean=0.1, reward_bound=0.0\n",
      "275: loss=0.893, reward_mean=0.0, reward_bound=0.0\n",
      "276: loss=0.860, reward_mean=0.0, reward_bound=0.0\n",
      "277: loss=1.128, reward_mean=0.1, reward_bound=0.0\n",
      "278: loss=0.747, reward_mean=0.1, reward_bound=0.0\n",
      "279: loss=0.918, reward_mean=0.1, reward_bound=0.0\n",
      "280: loss=0.849, reward_mean=0.0, reward_bound=0.0\n",
      "281: loss=0.825, reward_mean=0.1, reward_bound=0.0\n",
      "282: loss=0.959, reward_mean=0.0, reward_bound=0.0\n",
      "283: loss=0.953, reward_mean=0.0, reward_bound=0.0\n",
      "284: loss=0.895, reward_mean=0.0, reward_bound=0.0\n",
      "285: loss=0.731, reward_mean=0.1, reward_bound=0.0\n",
      "286: loss=0.893, reward_mean=0.0, reward_bound=0.0\n",
      "287: loss=0.979, reward_mean=0.0, reward_bound=0.0\n",
      "288: loss=0.880, reward_mean=0.1, reward_bound=0.0\n",
      "289: loss=0.955, reward_mean=0.1, reward_bound=0.0\n",
      "290: loss=0.864, reward_mean=0.0, reward_bound=0.0\n",
      "291: loss=0.849, reward_mean=0.0, reward_bound=0.0\n",
      "292: loss=0.875, reward_mean=0.0, reward_bound=0.0\n",
      "293: loss=0.794, reward_mean=0.0, reward_bound=0.0\n",
      "294: loss=1.107, reward_mean=0.0, reward_bound=0.0\n",
      "295: loss=0.861, reward_mean=0.0, reward_bound=0.0\n",
      "296: loss=0.911, reward_mean=0.0, reward_bound=0.0\n",
      "297: loss=0.791, reward_mean=0.1, reward_bound=0.0\n",
      "298: loss=0.858, reward_mean=0.0, reward_bound=0.0\n",
      "299: loss=0.893, reward_mean=0.1, reward_bound=0.0\n",
      "300: loss=0.904, reward_mean=0.1, reward_bound=0.0\n",
      "301: loss=0.828, reward_mean=0.0, reward_bound=0.0\n",
      "302: loss=0.889, reward_mean=0.0, reward_bound=0.0\n",
      "303: loss=0.801, reward_mean=0.1, reward_bound=0.0\n",
      "304: loss=0.908, reward_mean=0.1, reward_bound=0.0\n",
      "305: loss=0.924, reward_mean=0.1, reward_bound=0.0\n",
      "306: loss=0.907, reward_mean=0.0, reward_bound=0.0\n",
      "307: loss=0.947, reward_mean=0.0, reward_bound=0.0\n",
      "308: loss=0.899, reward_mean=0.0, reward_bound=0.0\n",
      "309: loss=0.871, reward_mean=0.0, reward_bound=0.0\n",
      "310: loss=0.756, reward_mean=0.1, reward_bound=0.0\n",
      "311: loss=0.863, reward_mean=0.1, reward_bound=0.0\n",
      "312: loss=0.866, reward_mean=0.0, reward_bound=0.0\n",
      "313: loss=0.818, reward_mean=0.0, reward_bound=0.0\n",
      "314: loss=0.817, reward_mean=0.1, reward_bound=0.0\n",
      "315: loss=0.827, reward_mean=0.0, reward_bound=0.0\n",
      "316: loss=0.977, reward_mean=0.0, reward_bound=0.0\n",
      "317: loss=0.705, reward_mean=0.0, reward_bound=0.0\n",
      "318: loss=0.805, reward_mean=0.0, reward_bound=0.0\n",
      "319: loss=0.835, reward_mean=0.1, reward_bound=0.0\n",
      "320: loss=0.667, reward_mean=0.0, reward_bound=0.0\n",
      "321: loss=0.764, reward_mean=0.0, reward_bound=0.0\n",
      "322: loss=0.707, reward_mean=0.1, reward_bound=0.0\n",
      "323: loss=0.646, reward_mean=0.0, reward_bound=0.0\n",
      "324: loss=0.681, reward_mean=0.0, reward_bound=0.0\n",
      "325: loss=0.703, reward_mean=0.0, reward_bound=0.0\n",
      "326: loss=0.704, reward_mean=0.0, reward_bound=0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327: loss=0.756, reward_mean=0.0, reward_bound=0.0\n",
      "328: loss=0.863, reward_mean=0.1, reward_bound=0.0\n",
      "329: loss=0.587, reward_mean=0.0, reward_bound=0.0\n",
      "330: loss=0.739, reward_mean=0.0, reward_bound=0.0\n",
      "331: loss=0.694, reward_mean=0.1, reward_bound=0.0\n",
      "332: loss=0.654, reward_mean=0.1, reward_bound=0.0\n",
      "333: loss=0.737, reward_mean=0.1, reward_bound=0.0\n",
      "334: loss=0.807, reward_mean=0.1, reward_bound=0.0\n",
      "335: loss=0.609, reward_mean=0.0, reward_bound=0.0\n",
      "336: loss=0.723, reward_mean=0.0, reward_bound=0.0\n",
      "337: loss=0.784, reward_mean=0.1, reward_bound=0.0\n",
      "338: loss=0.738, reward_mean=0.0, reward_bound=0.0\n",
      "339: loss=0.640, reward_mean=0.1, reward_bound=0.0\n",
      "340: loss=0.681, reward_mean=0.0, reward_bound=0.0\n",
      "341: loss=0.617, reward_mean=0.0, reward_bound=0.0\n",
      "342: loss=0.640, reward_mean=0.1, reward_bound=0.0\n",
      "343: loss=0.750, reward_mean=0.0, reward_bound=0.0\n",
      "344: loss=0.787, reward_mean=0.1, reward_bound=0.0\n",
      "345: loss=0.553, reward_mean=0.0, reward_bound=0.0\n",
      "346: loss=0.684, reward_mean=0.0, reward_bound=0.0\n",
      "347: loss=0.611, reward_mean=0.0, reward_bound=0.0\n",
      "348: loss=0.741, reward_mean=0.1, reward_bound=0.0\n",
      "349: loss=0.621, reward_mean=0.0, reward_bound=0.0\n",
      "350: loss=0.640, reward_mean=0.0, reward_bound=0.0\n",
      "351: loss=0.701, reward_mean=0.0, reward_bound=0.0\n",
      "352: loss=0.625, reward_mean=0.1, reward_bound=0.0\n",
      "353: loss=0.750, reward_mean=0.1, reward_bound=0.0\n",
      "354: loss=0.540, reward_mean=0.0, reward_bound=0.0\n",
      "355: loss=0.517, reward_mean=0.0, reward_bound=0.0\n",
      "356: loss=0.603, reward_mean=0.0, reward_bound=0.0\n",
      "357: loss=0.534, reward_mean=0.0, reward_bound=0.0\n",
      "358: loss=0.587, reward_mean=0.0, reward_bound=0.0\n",
      "359: loss=0.574, reward_mean=0.1, reward_bound=0.0\n",
      "360: loss=0.640, reward_mean=0.1, reward_bound=0.0\n",
      "361: loss=0.617, reward_mean=0.1, reward_bound=0.0\n",
      "362: loss=0.537, reward_mean=0.0, reward_bound=0.0\n",
      "363: loss=0.413, reward_mean=0.1, reward_bound=0.0\n",
      "364: loss=0.499, reward_mean=0.1, reward_bound=0.0\n",
      "365: loss=0.395, reward_mean=0.1, reward_bound=0.0\n",
      "366: loss=0.411, reward_mean=0.0, reward_bound=0.0\n",
      "367: loss=0.438, reward_mean=0.1, reward_bound=0.0\n",
      "368: loss=0.427, reward_mean=0.1, reward_bound=0.0\n",
      "369: loss=0.503, reward_mean=0.0, reward_bound=0.0\n",
      "370: loss=0.420, reward_mean=0.0, reward_bound=0.0\n",
      "371: loss=0.360, reward_mean=0.1, reward_bound=0.0\n",
      "372: loss=0.240, reward_mean=0.1, reward_bound=0.0\n",
      "373: loss=0.348, reward_mean=0.1, reward_bound=0.0\n",
      "374: loss=0.454, reward_mean=0.1, reward_bound=0.0\n",
      "375: loss=0.399, reward_mean=0.0, reward_bound=0.0\n",
      "376: loss=0.365, reward_mean=0.0, reward_bound=0.0\n",
      "377: loss=0.246, reward_mean=0.1, reward_bound=0.0\n",
      "378: loss=0.374, reward_mean=0.0, reward_bound=0.0\n",
      "379: loss=0.520, reward_mean=0.0, reward_bound=0.0\n",
      "380: loss=0.246, reward_mean=0.0, reward_bound=0.0\n",
      "381: loss=0.355, reward_mean=0.0, reward_bound=0.0\n",
      "382: loss=0.369, reward_mean=0.1, reward_bound=0.0\n",
      "383: loss=0.423, reward_mean=0.1, reward_bound=0.0\n",
      "384: loss=0.299, reward_mean=0.0, reward_bound=0.0\n",
      "385: loss=0.346, reward_mean=0.1, reward_bound=0.0\n",
      "386: loss=0.515, reward_mean=0.0, reward_bound=0.0\n",
      "387: loss=0.314, reward_mean=0.0, reward_bound=0.0\n",
      "388: loss=0.253, reward_mean=0.0, reward_bound=0.0\n",
      "389: loss=0.511, reward_mean=0.0, reward_bound=0.0\n",
      "390: loss=0.558, reward_mean=0.1, reward_bound=0.0\n",
      "391: loss=0.338, reward_mean=0.0, reward_bound=0.0\n",
      "392: loss=0.575, reward_mean=0.1, reward_bound=0.0\n",
      "393: loss=0.346, reward_mean=0.0, reward_bound=0.0\n",
      "394: loss=0.390, reward_mean=0.0, reward_bound=0.0\n",
      "395: loss=0.364, reward_mean=0.0, reward_bound=0.0\n",
      "396: loss=0.430, reward_mean=0.0, reward_bound=0.0\n",
      "397: loss=0.460, reward_mean=0.0, reward_bound=0.0\n",
      "398: loss=0.711, reward_mean=0.0, reward_bound=0.0\n",
      "399: loss=0.528, reward_mean=0.0, reward_bound=0.0\n",
      "400: loss=0.441, reward_mean=0.1, reward_bound=0.0\n",
      "401: loss=0.552, reward_mean=0.0, reward_bound=0.0\n",
      "402: loss=0.512, reward_mean=0.0, reward_bound=0.0\n",
      "403: loss=0.584, reward_mean=0.1, reward_bound=0.0\n",
      "404: loss=0.450, reward_mean=0.0, reward_bound=0.0\n",
      "405: loss=0.664, reward_mean=0.2, reward_bound=0.0\n",
      "406: loss=0.734, reward_mean=0.0, reward_bound=0.0\n",
      "407: loss=0.640, reward_mean=0.1, reward_bound=0.0\n",
      "408: loss=0.537, reward_mean=0.1, reward_bound=0.0\n",
      "409: loss=0.537, reward_mean=0.0, reward_bound=0.0\n",
      "410: loss=0.606, reward_mean=0.0, reward_bound=0.0\n",
      "411: loss=0.577, reward_mean=0.0, reward_bound=0.0\n",
      "412: loss=0.686, reward_mean=0.0, reward_bound=0.0\n",
      "413: loss=0.676, reward_mean=0.0, reward_bound=0.0\n",
      "414: loss=0.652, reward_mean=0.0, reward_bound=0.0\n",
      "415: loss=0.603, reward_mean=0.1, reward_bound=0.0\n",
      "416: loss=0.625, reward_mean=0.0, reward_bound=0.0\n",
      "417: loss=0.742, reward_mean=0.0, reward_bound=0.0\n",
      "418: loss=0.741, reward_mean=0.0, reward_bound=0.0\n",
      "419: loss=0.606, reward_mean=0.1, reward_bound=0.0\n",
      "420: loss=0.624, reward_mean=0.1, reward_bound=0.0\n",
      "421: loss=0.636, reward_mean=0.0, reward_bound=0.0\n",
      "422: loss=0.617, reward_mean=0.1, reward_bound=0.0\n",
      "423: loss=0.735, reward_mean=0.0, reward_bound=0.0\n",
      "424: loss=0.580, reward_mean=0.1, reward_bound=0.0\n",
      "425: loss=0.703, reward_mean=0.1, reward_bound=0.0\n",
      "426: loss=0.743, reward_mean=0.0, reward_bound=0.0\n",
      "427: loss=0.621, reward_mean=0.1, reward_bound=0.0\n",
      "428: loss=0.722, reward_mean=0.0, reward_bound=0.0\n",
      "429: loss=0.685, reward_mean=0.0, reward_bound=0.0\n",
      "430: loss=0.488, reward_mean=0.1, reward_bound=0.0\n",
      "431: loss=0.787, reward_mean=0.0, reward_bound=0.0\n",
      "432: loss=0.607, reward_mean=0.0, reward_bound=0.0\n",
      "433: loss=0.713, reward_mean=0.1, reward_bound=0.0\n",
      "434: loss=0.622, reward_mean=0.0, reward_bound=0.0\n",
      "435: loss=0.667, reward_mean=0.0, reward_bound=0.0\n",
      "436: loss=0.697, reward_mean=0.1, reward_bound=0.0\n",
      "437: loss=0.733, reward_mean=0.0, reward_bound=0.0\n",
      "438: loss=0.491, reward_mean=0.0, reward_bound=0.0\n",
      "439: loss=0.532, reward_mean=0.0, reward_bound=0.0\n",
      "440: loss=0.596, reward_mean=0.1, reward_bound=0.0\n",
      "441: loss=0.489, reward_mean=0.0, reward_bound=0.0\n",
      "442: loss=0.525, reward_mean=0.0, reward_bound=0.0\n",
      "443: loss=0.632, reward_mean=0.1, reward_bound=0.0\n",
      "444: loss=0.648, reward_mean=0.0, reward_bound=0.0\n",
      "445: loss=0.522, reward_mean=0.0, reward_bound=0.0\n",
      "446: loss=0.489, reward_mean=0.0, reward_bound=0.0\n",
      "447: loss=0.574, reward_mean=0.1, reward_bound=0.0\n",
      "448: loss=0.437, reward_mean=0.1, reward_bound=0.0\n",
      "449: loss=0.600, reward_mean=0.1, reward_bound=0.0\n",
      "450: loss=0.573, reward_mean=0.1, reward_bound=0.0\n",
      "451: loss=0.544, reward_mean=0.0, reward_bound=0.0\n",
      "452: loss=0.643, reward_mean=0.0, reward_bound=0.0\n",
      "453: loss=0.799, reward_mean=0.0, reward_bound=0.0\n",
      "454: loss=0.423, reward_mean=0.1, reward_bound=0.0\n",
      "455: loss=0.546, reward_mean=0.1, reward_bound=0.0\n",
      "456: loss=0.561, reward_mean=0.0, reward_bound=0.0\n",
      "457: loss=0.504, reward_mean=0.1, reward_bound=0.0\n",
      "458: loss=0.562, reward_mean=0.1, reward_bound=0.0\n",
      "459: loss=0.493, reward_mean=0.1, reward_bound=0.0\n",
      "460: loss=0.487, reward_mean=0.1, reward_bound=0.0\n",
      "461: loss=0.514, reward_mean=0.0, reward_bound=0.0\n",
      "462: loss=0.508, reward_mean=0.1, reward_bound=0.0\n",
      "463: loss=0.362, reward_mean=0.1, reward_bound=0.0\n",
      "464: loss=0.481, reward_mean=0.2, reward_bound=0.0\n",
      "465: loss=0.525, reward_mean=0.0, reward_bound=0.0\n",
      "466: loss=0.419, reward_mean=0.1, reward_bound=0.0\n",
      "467: loss=0.437, reward_mean=0.1, reward_bound=0.0\n",
      "468: loss=0.412, reward_mean=0.1, reward_bound=0.0\n",
      "469: loss=0.528, reward_mean=0.1, reward_bound=0.0\n",
      "470: loss=0.293, reward_mean=0.1, reward_bound=0.0\n",
      "471: loss=0.415, reward_mean=0.1, reward_bound=0.0\n",
      "472: loss=0.559, reward_mean=0.0, reward_bound=0.0\n",
      "473: loss=0.540, reward_mean=0.0, reward_bound=0.0\n",
      "474: loss=0.298, reward_mean=0.1, reward_bound=0.0\n",
      "475: loss=0.432, reward_mean=0.0, reward_bound=0.0\n",
      "476: loss=0.405, reward_mean=0.0, reward_bound=0.0\n",
      "477: loss=0.461, reward_mean=0.1, reward_bound=0.0\n",
      "478: loss=0.331, reward_mean=0.1, reward_bound=0.0\n",
      "479: loss=0.539, reward_mean=0.1, reward_bound=0.0\n",
      "480: loss=0.515, reward_mean=0.0, reward_bound=0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19396/695218647.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"-frozenlake-naive\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0miter_no\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterate_batchs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;31m#env.render()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mtrain_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_act\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward_m\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilter_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpercentile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19396/419579261.py\u001b[0m in \u001b[0;36miterate_batchs\u001b[1;34m(env, model, batch_size)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m#obs_v = onehotencoder(obs, env.observation_space.n)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mobs_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;31m#obs_v = torch.tensor(obs_v, dtype=torch.float32).unsqueeze(0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mactions_prob_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs_v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v1\"))\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "net = model(obs_size, n_neurons, n_actions)\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "writer = SummaryWriter(comment=\"-frozenlake-naive\")\n",
    "\n",
    "for iter_no, batch in enumerate(iterate_batchs(env, net, batch_size)):\n",
    "    #env.render()\n",
    "    train_obs, train_act, reward_b, reward_m = filter_batch(batch, percentile)\n",
    "    \n",
    "    #act_preds = net(torch.tensor(OHE(train_obs, e.observation_space.n), dtype=torch.float32))\n",
    "    act_preds = net(train_obs)\n",
    "    \n",
    "    loss = objective(act_preds, train_act)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (\n",
    "            iter_no, loss.item(), reward_m, reward_b))\n",
    "    writer.add_scalar(\"loss\", loss.item(), iter_no)\n",
    "    writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "    writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "    if reward_m > 199:\n",
    "        print(\"Solved!\")\n",
    "        break\n",
    "        \n",
    "writer.close()\n",
    "#e.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acf2e41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
