{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9886f05-46e1-4908-baf8-dd34d36ee809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from PIL import Image, ImageOps\n",
    "from tqdm.auto import tqdm\n",
    "import gymnasium as gym\n",
    "import cv2\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bb769db-5f12-4989-babd-0354bf58c078",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"ALE/SpaceInvaders-v5\"\n",
    "\n",
    "env = gym.make(env_id, render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26582e73-bf2c-4de7-8806-94038a43d59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(img, size=(128, 128)):\n",
    "    img = Image.fromarray(img)\n",
    "    img = ImageOps.grayscale(img).resize((size[0], size[1]))\n",
    "    img = np.array(img)\n",
    "    return torch.tensor(img, dtype=torch.float) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9edb42a-4501-4c65-831e-691b74389229",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(torch.nn.Module):\n",
    "    def __init__(self, in_dim=1, out_dim=env.action_space.n):\n",
    "        super(Policy, self).__init__()\n",
    "        self.conv_net = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_dim, 4, 3),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(4, 8, 3),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(8, 16, 3),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(16, 32, 3),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1152, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_net(x)\n",
    "        flattened = torch.flatten(conv_out, start_dim=1)\n",
    "        fc_out = self.fc(flattened)\n",
    "        return torch.nn.functional.softmax(fc_out, dim=1)\n",
    "        return fc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "058e3926-4215-4af7-bd5f-e157d95a54b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Policy().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "39db4f1d-4d33-452a-a1ac-21b6ec1d5df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "729350\n"
     ]
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4e89339c-d5c7-418c-863b-7426ecee1443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(rewards, log_probs, gamma, model, optimizer):\n",
    "    rewards = torch.tensor(rewards)\n",
    "    log_probs = torch.cat(log_probs).sum()\n",
    "    returns = rewards * torch.pow(gamma, torch.arange(len(rewards)))\n",
    "    returns = (returns.sum() - returns.mean()) / (returns.std()+1e-12)\n",
    "    loss = -1*returns*log_probs\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "\n",
    "    step = 0\n",
    "\n",
    "    return rewards, log_probs, step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b7165f36-8975-45c1-9b1c-f432f6f9b6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.95\n",
    "lr = 1e-2\n",
    "batch = 16\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7fb8e621-529d-4d15-847a-cfb56c9f8309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, env, n_episodes, gamma, batch):\n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        state, info = env.reset()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        episode_rewards = []\n",
    "        episode_log_probs = []\n",
    "\n",
    "        step = 0\n",
    "\n",
    "        while True:\n",
    "            step+=1\n",
    "            processed_state = process_image(state).unsqueeze(0).unsqueeze(0).to(device)\n",
    "            probs = model(processed_state)\n",
    "            m = Categorical(probs)\n",
    "            action = m.sample()\n",
    "            log_prob = m.log_prob(action)\n",
    "            n_state, reward, terminated, truncated, info = env.step(action.item())\n",
    "\n",
    "            if terminated or truncated:\n",
    "                episode_rewards, episode_log_probs, step = update_model(episode_rewards, episode_log_probs, gamma, model, optimizer)\n",
    "                break\n",
    "    \n",
    "            episode_rewards.append(reward)\n",
    "            episode_log_probs.append(log_prob)\n",
    "\n",
    "            if step == batch:\n",
    "                episode_rewards, episode_log_probs, step = update_model(episode_rewards, episode_log_probs, gamma, model, optimizer)\n",
    "                \n",
    "            state = n_state\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fde38139-345f-4cd8-aa85-91891b9423db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [10:19<00:00,  6.20s/it]\n"
     ]
    }
   ],
   "source": [
    "trained_model = train(model, optimizer, env, 100, gamma, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e155ad5b-4604-410a-9ac5-4b16e539e159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, n_eval_episodes, policy):\n",
    "    episode_rewards = []\n",
    "    for episode in tqdm(range(n_eval_episodes)):\n",
    "        state, info = env.reset()\n",
    "        step = 0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        total_rewards_ep = 0\n",
    "        \n",
    "        while True:\n",
    "            processed_state = process_image(state).unsqueeze(0).unsqueeze(0).to(device)\n",
    "            probs = policy(processed_state)\n",
    "            m = Categorical(probs)\n",
    "            action = m.sample()\n",
    "            log_prob = m.log_prob(action)\n",
    "            new_state, reward, terminated, truncated, info = env.step(action.item())\n",
    "            total_rewards_ep+= reward\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            state = new_state\n",
    "            \n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "    \n",
    "    return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0c3d674f-ebdd-4c98-a56d-741347f4ebca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:33<00:00,  3.33s/it]\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_agent(env, 10, trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0b6bd990-13a5-45c1-9d09-2b7633f02651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270.0, 0.0)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "306da5da-c839-48fd-96ab-2b2d4ed69ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... \t Episode: 1\t:   7%|███▍                                             | 7/100 [1:52:22<24:52:53, 963.16s/it]"
     ]
    }
   ],
   "source": [
    "state, info = env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "step = 0\n",
    "\n",
    "episode_reward = 0\n",
    "\n",
    "while True:\n",
    "    \n",
    "    processed_state = process_image(state).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        probs = trained_model(processed_state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        log_prob = m.log_prob(action)\n",
    "    n_state, reward, terminated, truncated, info = env.step(action.item())\n",
    "\n",
    "    frame = env.render()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    frame = cv2.resize(frame, (320, 420))\n",
    "    frame = cv2.putText(frame, f'Action taken: {action.item()}  Reward: {reward}', (10, 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "    cv2.imshow(\"gameplay\", frame)\n",
    "    pressedKey = cv2.waitKey(60) & 0xFF\n",
    "    if pressedKey == ord('q'):\n",
    "        break\n",
    "\n",
    "    episode_reward += reward\n",
    "\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "    state = n_state\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "print(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663a3d13-0a9e-4caf-b517-e8ba0564c85b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
